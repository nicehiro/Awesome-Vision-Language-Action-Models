#+title: Awesome Vision-Language-Action (VLA) Models
#+latex_header: usepackage{arev}


From RT-2 (Google DeepMind):

#+begin_quote
We propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, _we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens_. We refer to  such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2.
#+end_quote

Vision-Language-Action models typically utilize pre-trained vision-language models (VLMs) as their base and are fine-tuned to predict robotic actions. The models that do not rely on pre-trained VLMs are referred to here as *Robotic Foundation Models*.


* VLA Models

We categorize current VLA architectures into two broad families:

- *Single‑system VLAs.* A single vision–language model handles perception, reasoning, and action prediction. Continuous motor commands are first discretized by a learned action tokenizer (derived from the text tokenizer), and the model simply generates these tokens in sequence.

- *Dual‑system VLAs.* High‑level understanding and low‑level control are split. A vision–language backbone encodes the current images and instruction, while a dedicated action‑policy network produces continuous commands from those embeddings.

Two principal mechanisms are used to connect the vision–language backbone to the action head:

- *Special‑token-bridged.* During VLM fine‑tuning, a reserved token such as _<ACT>_ is appended; its final embedding is passed directly to the policy.

- *Feature‑pooling-bridged.* The full sequence of hidden states is aggregated—via max‑pooling, mean‑pooling, or learned attention—to yield a compact feature vector fed to the policy.

- *Cross-attention-bridged.* The action expert uses interleaved cross-attention layers to directly attend to the VLM's hidden representations throughout the forward pass.


** Single-system VLA

- *LoHoVLA*: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks, arxiv, May 31 2025. [[[http://arxiv.org/abs/2506.00411][Paper]]]

- *UniVLA*: Learning to Act Anywhere with  Task-centric Latent Actions, The University of Hong Kong, arxiv, May 9 2025. [[[http://arxiv.org/abs/2505.06111][Paper]]] [[[https://github.com/OpenDriveLab/UniVLA][Code]]]

- *3D-CAVLA*: 3D-CAVLA: Leveraging Depth and 3D Context to Generalize Vision–Language Action Models for Unseen Tasks, New York University, arxiv, May 9 2025. [[[http://arxiv.org/abs/2505.05800][Paper]]] [[[https://3d-cavla.github.io/][Website]]]

- *NORA*: NORA: A SMALL OPEN-SOURCED GENERALIST VISION LANGUAGE ACTION MODEL FOR EMBODIED TASKS, Singapore University of Technology and Design, arxiv, Apr 28 2025. [[[http://arxiv.org/abs/2504.19854][Paper]]]

- *CoT-VLA*: CoT-VLA: Visual Chain-of-Thought Reasoning for  Vision-Language-Action Models, NVIDIA & Stanford, arxiv, Mar 27 2025. [[[http://arxiv.org/abs/2503.22020][Paper]]] [[[https://cot-vla.github.io/][Website]]]

- *PD-VLA*: Accelerating Vision-Language-Action Model Integrated with Action  Chunking via Parallel Decoding, HKUST (GZ), arxiv, Mar 4 2025. [[[http://arxiv.org/abs/2503.02310][Paper]]]

- *VLAS*: VLAS: VISION-LANGUAGE-ACTION MODEL WITH  SPEECH INSTRUCTIONS FOR CUSTOMIZED ROBOT  MANIPULATION, Westlake University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.13508][Paper]]] [[[https://github.com/whichwhichgone/VLAS][Code]]]

- *VLA-Cache*: Towards Efficient Vision-Language-Action Model via Adaptive  Token Caching in Robotic Manipulation, University of Sydney, arxiv, Feb 4 2025. [[[http://arxiv.org/abs/2502.02175][Paper]]]

- *Spatial-VLA*: SpatialVLA: Exploring Spatial Representations for  Visual-Language-Action Models, Shanghai AI Lab, arxiv, Jan 28 2025. [[[https://arxiv.org/abs/2501.15830][Paper]]] [[[https://spatialvla.github.io][Website]]] [[[https://github.com/SpatialVLA/SpatialVLA][Code]]] [[[https://huggingface.co/collections/IPEC-COMMUNITY/foundation-vision-language-action-model-6795eb96a9c661f90236acbb][Model]]]

- *TRACEVLA*: TRACEVLA: VISUAL TRACE PROMPTING ENHANCES  SPATIAL-TEMPORAL AWARENESS FOR GENERALIST  ROBOTIC POLICIES, University of Maryland, arxiv, Dec 25 2024. [[[http://arxiv.org/abs/2412.10345][Paper]]]

- *OpenVLA*: OpenVLA: An Open-Source Vision-Language-Action Model, Stanford University & UC Berkeley & Toyota Research Insititute, arxiv, Jun 13 2024. [[[https://openvla.github.io][Website]]] [[[http://arxiv.org/abs/2412.03293][Paper]]] [[[https://github.com/openvla/openvla][Code]]] [[[https://huggingface.co/openvla][Model]]]

- *RT-2*: RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Google DeepMind, July 28 2023.

** Dual-system VLA

*** Special-token-bridged

- *OpenHelix*: A Short Survey, Empirical Analysis, and Open-Source Dual-System  VLA Model for Robotic Manipulation, Westlake University, arxiv, May 6 2025. [[[http://arxiv.org/abs/2505.03912][Paper]]]

- *MoLe-VLA*: MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via  Mixture-of-Layers for Efficient Robot Manipulation, Nanjing University & HK PolyU & Peking University, Mar 26 2025. [[[http://arxiv.org/abs/2503.20384][Paper]]] [[[https://sites.google.com/view/mole-vla][Website]]] [[[https://github.com/RoyZry98/MoLe-VLA-Pytorch/][Code]]]

- *FuSe*: Beyond Sight: Finetuning Generalist Robot Policies with  Heterogeneous Sensors via Language Grounding, UC Berkeley, arxiv, Jan 8 2025. [[[https://fuse-model.github.io/][Website]]] [[[http://arxiv.org/abs/2501.04693][Paper]]] [[[https://github.com/fuse-model/FuSe][Code]]] [[[https://huggingface.co/datasets/oier-mees/FuSe][Model]]]

- *Diffusion-VLA*: Diffusion-VLA:  Scaling Robot Foundation Models via Unified Diffusion and Autoregression, East China Normal University, arxiv, Dec 4 2024. [[[https://diffusion-vla.github.io/][Website]]] [[[http://arxiv.org/abs/2412.03293][Paper]]]

- *CogACT*: CogACT: A Foundational Vision-Language-Action Model for Synergizing  Cognition and Action in Robotic Manipulation, Tsinghua University, arxiv, Nov 29 2024. [[[http://arxiv.org/abs/2411.19650][Paper]]] [[[https://cogact.github.io][Website]]] [[[https://github.com/microsoft/CogACT][Code]]] [[[https://huggingface.co/CogACT][Model]]]

*** Feature-pooling-bridged

- *OneTwoVLA*: A Unified Vision-Language-Action Model with Adaptive Reasoning, Tsinghua & Shanghai Qi Zhi & Shanghai AI Lab, arxiv, May 17 2025. [[[http://arxiv.org/abs/2505.11917][Paper]]] [[[https://one-two-vla.github.io][Website]]] [[[https://github.com/Fanqi-Lin/OneTwoVLA][Code]]] [[[https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset][Data]]]

- *Hi Robot*: Hi Robot: Open-Ended Instruction Following with Hierarchical  Vision-Language-Action Models, Physical Intelligence & Stanford University, arxiv, Feb 26 2025. [[[http://arxiv.org/abs/2502.19417][Paper]]] [[[https://www.pi.website/research/hirobot][Website]]]

- *ChatVLA*: ChatVLA: Unified Multimodal Understanding and Robot Control  with Vision-Language-Action Model, Midea Group & East China Normal University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.14420][Paper]]] [[[https://chatvla.github.io/][Website]]]

- *DexVLA*: DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control, Midea Group & East China Normal University, arxiv, Feb 9 2025. [[[http://arxiv.org/abs/2502.05855][Paper]]] [[[https://dex-vla.github.io/][Website]]] [[[https://github.com/lesjie-wen/dexvla][Code]]]

- *UP-VLA*: A Unified Understanding and Prediction Model for Embodied Agent, Tsinghua University & Shanghai Qi Zhi Institute, arxiv, Feb 3 2025. [[[http://arxiv.org/abs/2501.18867][Paper]]]

- *iRe-VLA*: Improving Vision-Language-Action Model with  Online Reinforcement Learning, Tsinghua University & Shanghai Qi Zhi Institute, arxiv, Jan 28 2025. [[[http://arxiv.org/abs/2501.16664][Paper]]]

- *FAST*: FAST: Efficient Action Tokenization for  Vision-Language-Action Models, Physical Intelligence & UC Berkeley & Stanford, arxiv, Jan 16 2025. [[[https://pi.website/research/fast][Website]]] [[[http://arxiv.org/abs/2501.09747][Paper]]] [[[https://huggingface.co/physical-intelligence/fast][Tokenizer]]] [[[https://github.com/Physical-Intelligence/openpi][Code]]]

- *DeeR-VLA*: DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution, Tsinghua University, NeurIPS 24. [[[https://openreview.net/forum?id=QKp3nhPU41&referrer=%5Bthe%20profile%20of%20Yizeng%20Han%5D(%2Fprofile%3Fid%3D~Yizeng_Han1)][Paper]]] [[[https://github.com/yueyang130/DeeR-VLA][Website]]] [[[https://github.com/yueyang130/DeeR-VLA][Code]]]


*** Cross-attention-bridged

- *SmolVLA*: A vision-language-action model for affordable and efficient robotics, Hugging Face, arxiv, Jun 4 2025. [[[http://arxiv.org/abs/2506.01844][Paper]]] [[[https://huggingface.co/blog/smolvla][Website]]] [[[https://huggingface.co/blog/smolvla][Model]]]

- $\pi_{0.5}$: $\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization, Physical Intelligence, arxiv, Apr 22 2025. [[[http://arxiv.org/abs/2504.16054][Paper]]] [[[https://www.pi.website/blog/pi05][Website]]]

- $\pi_0$: $\pi_0$: A Vision-Language-Action Flow Model for  General Robot Control, Physical Intelligence, arxiv, Oct 31 2024. [[[https://physicalintelligence.company/blog/pi0][Website]]] [[[http://arxiv.org/abs/2410.24164][Paper]]] [[[https://github.com/Physical-Intelligence/openpi][Code]]]


** Others

- *HybridVLA*: HybridVLA: Collaborative Diffusion and Autoregression in  a Unified Vision-Language-Action Model, Peking University, Mar 13 2025. [[[http://arxiv.org/abs/2503.10631][Paper]]] [[[https://hybrid-vla.github.io][Website]]] [[[https://github.com/PKU-HMI-Lab/Hybrid-VLA][Code]]] *


** For Humanoid Robots

- *GR00T N1*: GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, NVIDIA, Mar 27 2025. [[[http://arxiv.org/abs/2503.14734][Paper]]] [[[https://developer.nvidia.com/isaac/gr00t][Website]]] [[[https://github.com/NVIDIA/Isaac-GR00T][Code]]] [[[https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim][Dataset]]]

- *GO-1*: AgiBot World Colosseo: Large-scale Manipulation Platform  for Scalable and Intelligent Embodied Systems, AgiBot-World (Shanghai AI Lab & AgiBot Inc.), AgiBot World, Mar 10 2025. [[[https://agibot-world.com/blog/go1#:~:text=Paper:-,agibot_go1.pdf][Paper]]] [[[https://agibot-world.com][Website]]] [[[https://github.com/OpenDriveLab/Agibot-World][Code]]] [[[https://huggingface.co/agibot-world][Model]]]

- *Humanoid-VLA*: Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration, Westlake University & Zhejiang University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.14795][Paper]]]

- *NAVILA*: NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION, UC San Diego, arxiv, Dec 5 2024. [[[https://navila-bot.github.io/][Website]]] [[[http://arxiv.org/abs/2412.04453][Paper]]]


* Finetuning VLA Models

- Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better, Physical Intelligence, May 29 2025. [[[http://arxiv.org/abs/2505.23705][Paper]]] [[[https://pi.website/research/knowledge_insulation][Website]]]

- What Can RL Bring to VLA Generalization? An Empirical Study, Tsinghua, arxiv, May 26 2025. [[[http://arxiv.org/abs/2505.19789][Paper]]] [[[https://rlvla.github.io][Website]]] [[[https://github.com/gen-robot/RL4VLA][Code]]]

- *VLA-RL*: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning

- *OFT*: Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success, Stanford, arxiv, Apr 28 2025. [[[https://arxiv.org/abs/2502.19645][Paper]]] [[[https://openvla-oft.github.io/][Website]]] [[[https://github.com/moojink/openvla-oft][Code]]] [[[https://huggingface.co/moojink?search_models=oft][Model]]]


* Robotic Foundation Models

- *OTTER*: OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction, UC Berkeley, arxiv, Mar 5 2025. [[[http://arxiv.org/abs/2503.03734][Paper]]] [[[https://ottervla.github.io/][Website]]] [[[https://github.com/FangchenLiu/otter_jax][Code JAX]]] [[[https://github.com/Max-Fu/otter][Code Torch]]]

- *Octo*: Octo: An Open-Source Generalist Robot Policy. UC Berkeley, arxiv, May 20 2024. [[[https://arxiv.org/abs/2405.12213][Paper]]] [[[https://octo-models.github.io][Website]]] [[[https://github.com/octo-models/octo][Code]]] [[[https://huggingface.co/rail-berkeley][Model]]]
