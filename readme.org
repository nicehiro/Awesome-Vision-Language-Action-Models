#+title: Awesome Vision-Language-Action (VLA) Models
#+latex_header: usepackage{arev}


- *HybridVLA*: HybridVLA: Collaborative Diffusion and Autoregression in  a Unified Vision-Language-Action Model, Peking University, Mar 13 2025. [[[http://arxiv.org/abs/2503.10631][Paper]]] [[[https://hybrid-vla.github.io][Website]]] [[[https://github.com/PKU-HMI-Lab/Hybrid-VLA][Code]]]

- *OTTER*: OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction, UC Berkeley, arxiv, Mar 5 2025. [[[http://arxiv.org/abs/2503.03734][Paper]]] [[[https://ottervla.github.io/][Website]]] [[[https://github.com/FangchenLiu/otter_jax][Code JAX]]] [[[https://github.com/Max-Fu/otter][Code Torch]]]

- *PD-VLA*: Accelerating Vision-Language-Action Model Integrated with Action  Chunking via Parallel Decoding, HKUST (GZ), arxiv, Mar 4 2025. [[[http://arxiv.org/abs/2503.02310][Paper]]]

- *ChatVLA*: ChatVLA: Unified Multimodal Understanding and Robot Control  with Vision-Language-Action Model, Midea Group & East China Normal University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.14420][Paper]]] [[[https://chatvla.github.io/][Website]]]

- *VLAS*: VLAS: VISION-LANGUAGE-ACTION MODEL WITH  SPEECH INSTRUCTIONS FOR CUSTOMIZED ROBOT  MANIPULATION, Westlake University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.13508][Paper]]] [[[https://github.com/whichwhichgone/VLAS][Code]]]

- *DexVLA*: DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control, Midea Group & East China Normal University, arxiv, Feb 9 2025. [[[http://arxiv.org/abs/2502.05855][Paper]]] [[[https://dex-vla.github.io/][Website]]] [[[https://github.com/lesjie-wen/dexvla][Code]]]

- *ConRFT*: ConRFT: A Reinforced Fine-tuning Method for  VLA Models via Consistency Policy, Chinese Academy of Sciences, arxiv, Feb 8 2025. [[[http://arxiv.org/abs/2502.05450][Paper]]] [[[https://cccedric.github.io/conrft/][Website]]]

- *VLA-Cache*: Towards Efficient Vision-Language-Action Model via Adaptive  Token Caching in Robotic Manipulation, University of Sydney, arxiv, Feb 4 2025. [[[http://arxiv.org/abs/2502.02175][Paper]]]

- *UP-VLA*: A Unified Understanding and Prediction Model for Embodied Agent, Tsinghua University & Shanghai Qi Zhi Institute, arxiv, Feb 3 2025. [[[http://arxiv.org/abs/2501.18867][Paper]]]

- *iRe-VLA*: Improving Vision-Language-Action Model with  Online Reinforcement Learning, Tsinghua University & Shanghai Qi Zhi Institute, arxiv, Jan 28 2025. [[[http://arxiv.org/abs/2501.16664][Paper]]]

- *FAST*: FAST: Efficient Action Tokenization for  Vision-Language-Action Models, Physical Intelligence & UC Berkeley & Stanford, arxiv, Jan 16 2025. [[[https://pi.website/research/fast][Website]]] [[[http://arxiv.org/abs/2501.09747][Paper]]] [[[https://huggingface.co/physical-intelligence/fast][Tokenizer]]] [[[https://github.com/Physical-Intelligence/openpi][Code]]]

- *FuSe*: Beyond Sight: Finetuning Generalist Robot Policies with  Heterogeneous Sensors via Language Grounding, UC Berkeley, arxiv, Jan 8 2025. [[[https://fuse-model.github.io/][Website]]] [[[http://arxiv.org/abs/2501.04693][Paper]]] [[[https://github.com/fuse-model/FuSe][Code]]] [[[https://huggingface.co/datasets/oier-mees/FuSe][Model]]]

- *TRACEVLA*: TRACEVLA: VISUAL TRACE PROMPTING ENHANCES  SPATIAL-TEMPORAL AWARENESS FOR GENERALIST  ROBOTIC POLICIES, University of Maryland, arxiv, Dec 25 2024. [[[http://arxiv.org/abs/2412.10345][Paper]]]

- *EMMA-X*: EMMA-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Singapore University of Technology and Design, Dec 17 2024. [[[https: //declare-lab.github.io/Emma-X/][Website]]] [[[http://arxiv.org/abs/2412.11974][Paper]]] [[[https://github.com/declare-lab/Emma-X][Code]]]

- *Diffusion-VLA*: Diffusion-VLA:  Scaling Robot Foundation Models via Unified Diffusion and Autoregression, East China Normal University, arxiv, Dec 4 2024. [[[https://diffusion-vla.github.io/][Website]]] [[[http://arxiv.org/abs/2412.03293][Paper]]]

- $\pi_0$: $\pi_0$: A Vision-Language-Action Flow Model for  General Robot Control, Physical Intelligence, arxiv, Oct 31 2024. [[[https://physicalintelligence.company/blog/pi0][Website]]] [[[http://arxiv.org/abs/2410.24164][Paper]]] [[[https://github.com/Physical-Intelligence/openpi][Code]]]

- *OpenVLA*: OpenVLA: An Open-Source Vision-Language-Action Model, Stanford University & UC Berkeley & Toyota Research Insititute, arxiv, Jun 13 2024. [[[https://openvla.github.io][Website]]] [[[http://arxiv.org/abs/2412.03293][Paper]]] [[[https://github.com/openvla/openvla][Code]]] [[[https://huggingface.co/openvla][Model]]]


** For Humanoid Robots

- *NAVILA*: NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION, UC San Diego, arxiv, Dec 5 2024. [[[https://navila-bot.github.io/][Website]]] [[[http://arxiv.org/abs/2412.04453][Paper]]]

- *Humanoid-VLA*: Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration, Westlake University & Zhejiang University, arxiv, Feb 21 2025. [[[http://arxiv.org/abs/2502.14795][Paper]]]

- *GO-1*: AgiBot World Colosseo: Large-scale Manipulation Platform  for Scalable and Intelligent Embodied Systems, AgiBot-World (Shanghai AI Lab & AgiBot Inc.), AgiBot World, Mar 10 2025. [[[https://agibot-world.com/blog/go1#:~:text=Paper:-,agibot_go1.pdf][Paper]]] [[[https://agibot-world.com][Website]]] [[[https://github.com/OpenDriveLab/Agibot-World][Code]]] [[[https://huggingface.co/agibot-world][Model]]]
